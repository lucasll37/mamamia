version: '3.8'

services:
  # Manager Node
  manager:
    build:
      context: .
      dockerfile: deploy/docker/manager.Dockerfile
    container_name: ml-inference-manager
    hostname: manager
    ports:
      - "50051:50051"  # gRPC
      - "9090:9090"    # Prometheus metrics
    volumes:
      - ./data/manager:/data
      - ./configs/manager_config.yaml:/etc/ml-inference/manager_config.yaml:ro
    environment:
      - CONFIG_PATH=/etc/ml-inference/manager_config.yaml
      - LOG_LEVEL=info
    networks:
      - ml-inference-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/app/bin/health_check", "manager"]
      interval: 30s
      timeout: 10s
      retries: 3
  
  # Worker Node 1 (CPU)
  worker-cpu-1:
    build:
      context: .
      dockerfile: deploy/docker/worker.Dockerfile
    container_name: ml-inference-worker-cpu-1
    hostname: worker-cpu-1
    ports:
      - "50052:50052"  # gRPC
    volumes:
      - ./data/worker-cpu-1:/data
      - ./configs/worker_config.yaml:/etc/ml-inference/worker_config.yaml:ro
      - ./models:/models:ro
    environment:
      - CONFIG_PATH=/etc/ml-inference/worker_config.yaml
      - WORKER_ID=worker-cpu-1
      - MANAGER_ADDRESS=manager:50051
      - ENABLE_GPU=false
      - LOG_LEVEL=info
    networks:
      - ml-inference-net
    depends_on:
      - manager
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
  
  # Worker Node 2 (GPU) - Requires nvidia-docker
  worker-gpu-1:
    build:
      context: .
      dockerfile: deploy/docker/worker.Dockerfile
      args:
        ENABLE_GPU: "true"
    container_name: ml-inference-worker-gpu-1
    hostname: worker-gpu-1
    ports:
      - "50053:50052"  # gRPC
    volumes:
      - ./data/worker-gpu-1:/data
      - ./configs/worker_config.yaml:/etc/ml-inference/worker_config.yaml:ro
      - ./models:/models:ro
    environment:
      - CONFIG_PATH=/etc/ml-inference/worker_config.yaml
      - WORKER_ID=worker-gpu-1
      - MANAGER_ADDRESS=manager:50051
      - ENABLE_GPU=true
      - GPU_DEVICE_ID=0
      - LOG_LEVEL=info
    networks:
      - ml-inference-net
    depends_on:
      - manager
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  
  # API Gateway
  api-gateway:
    build:
      context: .
      dockerfile: deploy/docker/api-gateway.Dockerfile
    container_name: ml-inference-api-gateway
    hostname: api-gateway
    ports:
      - "8000:8000"  # REST API
    volumes:
      - ./configs/api_gateway_config.yaml:/etc/ml-inference/api_gateway_config.yaml:ro
    environment:
      - CONFIG_PATH=/etc/ml-inference/api_gateway_config.yaml
      - MANAGER_ADDRESS=manager
      - MANAGER_PORT=50051
      - LOG_LEVEL=info
      - WORKERS=4
    networks:
      - ml-inference-net
    depends_on:
      - manager
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
  
  # Web UI
  web-ui:
    build:
      context: ./web
      dockerfile: ../deploy/docker/web.Dockerfile
    container_name: ml-inference-web-ui
    hostname: web-ui
    ports:
      - "3000:80"  # Nginx
    environment:
      - REACT_APP_API_URL=http://localhost:8000
      - REACT_APP_WS_URL=ws://localhost:8000
    networks:
      - ml-inference-net
    depends_on:
      - api-gateway
    restart: unless-stopped
  
  # Prometheus (Optional - for monitoring)
  prometheus:
    image: prom/prometheus:latest
    container_name: ml-inference-prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./deploy/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - ml-inference-net
    restart: unless-stopped
  
  # Grafana (Optional - for visualization)
  grafana:
    image: grafana/grafana:latest
    container_name: ml-inference-grafana
    ports:
      - "3001:3000"
    volumes:
      - ./deploy/monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./deploy/monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
      - grafana-data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    networks:
      - ml-inference-net
    depends_on:
      - prometheus
    restart: unless-stopped

networks:
  ml-inference-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  prometheus-data:
  grafana-data:
